
## Evaluation settings ##
#########################

# Name of the model to be evaluated, corresponding to the name of the python file containing model definition (without extension)
model_name: llama2hf

# 
#model_parameters: {model: gpt-3.5-turbo-1106, temperature: 0}

# Benchmark selection:
benchmarks:

- name: sqad
  use: true
  local: true  # true: load from local files, false: load from Hugging Face cloud or cache

- name: squad
  use: false
  local: true

- name: anli
  use: false
  local: true

- name: czech_news  # huge number of test samples makes it expensive to evaluate
  use: false
  local: true 

# Limit the number of used examples for each benchmark (debugging purposes)
stop_idx: null  # Set to null for no limit
