
## Evaluation settings ##
#########################

# Name of the model to be evaluated, corresponding to the name of the python file containing model definition (without extension)
model_name: llama2_hf

# Dictionary with optional parameters to be passed to the get_llm function. Can be ommited.
#model_parameters: {model_id: gpt-3.5-turbo-1106}  # chat_openai
#model_parameters: {model_id: gpt-4-1106-preview}  # chat_openai
#model_parameters: {model_id: meta-llama/Llama-2-7b-chat-hf, temperature: 0.1}  # llama2_hf
#model_parameters: {temperature: 0.5}

# Benchmark selection:
benchmarks:

- name: sqad
  use: false
  local: true  # true: load from local files, false: load from Hugging Face cloud or cache

- name: squad
  use: false
  local: true

- name: anli
  use: true
  local: true

- name: czech_news  # huge number of test samples makes it expensive to evaluate
  use: false
  local: true 

# Limit the number of used examples for each benchmark (debugging purposes)
stop_idx: 2  # Set to null for no limit
