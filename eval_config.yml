
## Evaluation settings ##
#########################

# Name of the model to be evaluated, corresponding to the name of the python file containing model definition (without extension)
model_name: ollama_raw

# Dictionary with optional parameters to be passed to the get_llm function. Can be ommited.
#model_parameters: {model_id: gpt-3.5-turbo-1106}  # chat_openai
#model_parameters: {model_id: gpt-4-1106-preview}  # chat_openai
#model_parameters: {model_id: meta-llama/Llama-2-13b-chat-hf}  # llama2_hf
#model_parameters: {temperature: 0.5} # any
model_parameters: {model_id: llama2, url: http://dgx-5:11434, temperature: 0.} # ollama_raw

# Benchmark selection:
benchmarks:

- name: sqad
  use: false
  local: true  # true: load from local files, false: load from Hugging Face cloud or cache

- name: squad
  use: false
  local: true

- name: anli
  use: false
  local: true

- name: snli
  use: false
  local: true

- name: ctkfacts
  use: false
  local: true

- name: klokanek
  use: true
  local: true

- name: czech_news  # huge number of test samples makes it expensive to evaluate
  use: false
  local: true 

# Limit the number of used examples for each benchmark (debugging purposes)
stop_idx: null  # Set to null for no limit
