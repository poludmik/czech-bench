
## Evaluation settings ##
#########################

# Name of the model to be evaluated, corresponding to the name of the python file containing model definition (without extension)
model_name: ollama_raw
#model_name: chat_openai

# Dictionary with optional parameters to be passed to the get_llm function. Can be ommited.
#model_parameters: {model_id: gpt-3.5-turbo-1106}  # chat_openai
#model_parameters: {model_id: gpt-4-1106-preview}  # chat_openai
#model_parameters: {model_id: meta-llama/Llama-2-13b-chat-hf}  # llama2_hf
#model_parameters: {temperature: 0.5} # any
model_parameters: {model_id: llama2, url: http://amd-01:11434, temperature: 0.} # ollama_raw

# Benchmark selection:
benchmarks:
# Core Benchmarks:
- name: anli
  use: true
  local: true  # true: load from local files, false: load from Hugging Face cloud or cache

- name: arc_challenge
  use: true
  local: true

- name: arc_easy
  use: true
  local: true

- name: ctkfacts
  use: true
  local: true

- name: mall_reviews
  use: true
  local: true

- name: mmlu
  use: true
  local: true

- name: klokanek
  use: true
  local: true

- name: sqad
  use: true
  local: true

- name: squad
  use: true
  local: true

# Additional Benchmarks:
- name: czech_news
  use: true
  local: true 

- name: facebook_comments
  use: true
  local: true

- name: snli
  use: true
  local: true

- name: subjectivity
  use: true
  local: true

# English Benchmarks:
- name: arc_challenge_en
  use: true
  local: false

- name: arc_easy_en
  use: true
  local: false

- name: mmlu_en
  use: true
  local: false

- name: truthfulqa_en
  use: true
  local: false

# Limit the number of used examples for each benchmark or subtask (debugging purposes)
stop_idx: 10  # Set to null for no limit
